#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIå¢å¼ºçš„è‡ªåŠ¨åŒ–é‡‘èæ–°é—»æ’­æŠ¥ç³»ç»Ÿ
ç»“åˆç¨‹åºè‡ªåŠ¨æŠ“å– + AIå¤§æ¨¡å‹æ™ºèƒ½åˆ†æ
ç›‘æ§å„å¤§å¹³å°çš„æ–°é—»åŠ¨æ€ï¼Œåˆ†æå¯¹è‚¡å¸‚å’ŒåŠ å¯†è´§å¸çš„å½±å“
"""

import requests
import tweepy
import time
import json
import re
import openai
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional
import logging
from dataclasses import dataclass
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import anthropic  # Claude API
from openai import AsyncOpenAI  # OpenAI API
import os
import sys

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥æ–°æ¶æ„ç»„ä»¶
try:
    import sys
    import os
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
    from config_manager import ConfigManager
    from free_data_collector import FreeDataCollector
    ENHANCED_MODE = True
    logger.info("å¢å¼ºæ¨¡å¼å·²å¯ç”¨ï¼šæ”¯æŒå…è´¹æ•°æ®æº")
except ImportError as e:
    logger.warning(f"æ— æ³•å¯¼å…¥å¢å¼ºç»„ä»¶: {e}")
    logger.info("ä½¿ç”¨åŸºç¡€æ¨¡å¼ï¼šä»…æ”¯æŒAPIæ•°æ®æº")
    ENHANCED_MODE = False

@dataclass
class AIAnalysisResult:
    """AIåˆ†æç»“æœæ•°æ®ç»“æ„"""
    impact_score: float
    market_prediction: str
    trading_suggestion: str
    sentiment: str
    confidence: float
    key_points: List[str]

@dataclass
class NewsItem:
    """æ–°é—»é¡¹ç›®æ•°æ®ç»“æ„"""
    timestamp: str
    source: str
    title: str
    content: str
    url: str
    ai_analysis: Optional[AIAnalysisResult] = None
    
class AIFinancialAnalyzer:
    """AIé‡‘èåˆ†æå™¨ - æ”¯æŒå¤šç§å¤§æ¨¡å‹"""
    
    def __init__(self, model_config: Dict):
        self.model_config = model_config
        self.model_type = model_config.get('type', 'openai')
        
        # åˆå§‹åŒ–ä¸åŒçš„AIå®¢æˆ·ç«¯
        if self.model_type == 'openai':
            self.client = AsyncOpenAI(api_key=model_config['api_key'])
            self.model_name = model_config.get('model', 'gpt-4')
        elif self.model_type == 'claude':
            self.client = anthropic.Anthropic(api_key=model_config['api_key'])
            self.model_name = model_config.get('model', 'claude-3-sonnet-20240229')
        elif self.model_type == 'custom':
            # æ”¯æŒè‡ªå®šä¹‰APIç«¯ç‚¹ï¼ˆå¦‚æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼‰
            self.api_url = model_config['api_url']
            self.headers = {'Authorization': f"Bearer {model_config['api_key']}"}
    
    async def analyze_news_with_ai(self, news_content: str, news_source: str) -> AIAnalysisResult:
        """ä½¿ç”¨AIåˆ†ææ–°é—»"""
        
        # æ„å»ºåˆ†ææç¤ºè¯
        analysis_prompt = self._build_analysis_prompt(news_content, news_source)
        
        try:
            if self.model_type == 'openai':
                result = await self._analyze_with_openai(analysis_prompt)
            elif self.model_type == 'claude':
                result = await self._analyze_with_claude(analysis_prompt)
            elif self.model_type == 'custom':
                result = await self._analyze_with_custom_api(analysis_prompt)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
            
            return self._parse_ai_response(result)
            
        except Exception as e:
            logger.error(f"AIåˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤åˆ†æç»“æœ
            return AIAnalysisResult(
                impact_score=0.5,
                market_prediction="AIåˆ†ææš‚æ—¶æ— æ³•å®Œæˆï¼Œå»ºè®®äººå·¥åˆ¤æ–­",
                trading_suggestion="è¯·è°¨æ…æŠ•èµ„ï¼Œç­‰å¾…æ›´å¤šä¿¡æ¯",
                sentiment="neutral",
                confidence=0.3,
                key_points=["AIåˆ†æå¼‚å¸¸"]
            )
    
    def _build_analysis_prompt(self, content: str, source: str) -> str:
        """æ„å»ºAIåˆ†ææç¤ºè¯"""
        return f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹æ–°é—»å¯¹è‚¡å¸‚å’ŒåŠ å¯†è´§å¸å¸‚åœºçš„å½±å“ï¼š

æ–°é—»æ¥æº: {source}
æ–°é—»å†…å®¹: {content}

è¯·ä»ä»¥ä¸‹å‡ ä¸ªç»´åº¦è¿›è¡Œåˆ†æï¼Œå¹¶ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š

1. å½±å“è¯„åˆ† (impact_score): 0-1ä¹‹é—´çš„æ•°å€¼ï¼Œè¡¨ç¤ºå¯¹å¸‚åœºçš„å½±å“ç¨‹åº¦
2. å¸‚åœºé¢„æµ‹ (market_prediction): è¯¦ç»†åˆ†æå¯¹è‚¡å¸‚ã€åŠ å¯†è´§å¸å¸‚åœºçš„å…·ä½“å½±å“
3. äº¤æ˜“å»ºè®® (trading_suggestion): åŸºäºåˆ†æç»™å‡ºçš„äº¤æ˜“å»ºè®®
4. æƒ…æ„Ÿå€¾å‘ (sentiment): positive/negative/neutral
5. ä¿¡å¿ƒåº¦ (confidence): 0-1ä¹‹é—´ï¼Œè¡¨ç¤ºåˆ†æçš„å¯ä¿¡åº¦
6. å…³é”®è¦ç‚¹ (key_points): 3-5ä¸ªå…³é”®åˆ†æè¦ç‚¹çš„åˆ—è¡¨

è¯·ç¡®ä¿åˆ†æå®¢è§‚ã€ä¸“ä¸šï¼Œå¹¶åŒ…å«é£é™©æç¤ºã€‚è¿”å›æ ¼å¼ï¼š
{{
    "impact_score": 0.7,
    "market_prediction": "...",
    "trading_suggestion": "...",
    "sentiment": "negative",
    "confidence": 0.8,
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2", "è¦ç‚¹3"]
}}

å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    async def _analyze_with_openai(self, prompt: str) -> str:
        """ä½¿ç”¨OpenAI APIåˆ†æ"""
        response = await self.client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿åˆ†ææ–°é—»å¯¹å¸‚åœºçš„å½±å“ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )
        return response.choices[0].message.content
    
    async def _analyze_with_claude(self, prompt: str) -> str:
        """ä½¿ç”¨Claude APIåˆ†æ"""
        response = self.client.messages.create(
            model=self.model_name,
            max_tokens=1000,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return response.content[0].text
    
    async def _analyze_with_custom_api(self, prompt: str) -> str:
        """ä½¿ç”¨è‡ªå®šä¹‰APIåˆ†æ"""
        async with aiohttp.ClientSession() as session:
            payload = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.3,
                "max_tokens": 1000
            }
            
            async with session.post(
                self.api_url, 
                json=payload, 
                headers=self.headers
            ) as response:
                result = await response.json()
                return result.get('choices', [{}])[0].get('message', {}).get('content', '')
    
    def _parse_ai_response(self, response: str) -> AIAnalysisResult:
        """è§£æAIå“åº”"""
        try:
            # å°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                data = json.loads(json_str)
                
                return AIAnalysisResult(
                    impact_score=float(data.get('impact_score', 0.5)),
                    market_prediction=data.get('market_prediction', ''),
                    trading_suggestion=data.get('trading_suggestion', ''),
                    sentiment=data.get('sentiment', 'neutral'),
                    confidence=float(data.get('confidence', 0.5)),
                    key_points=data.get('key_points', [])
                )
        except Exception as e:
            logger.error(f"è§£æAIå“åº”å¤±è´¥: {e}")
        
        # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå°è¯•æ–‡æœ¬è§£æ
        return self._parse_text_response(response)
    
    def _parse_text_response(self, response: str) -> AIAnalysisResult:
        """è§£ææ–‡æœ¬æ ¼å¼çš„AIå“åº”"""
        # åŸºäºå…³é”®è¯çš„ç®€å•è§£æ
        impact_score = 0.5
        sentiment = 'neutral'
        
        response_lower = response.lower()
        if any(word in response_lower for word in ['å¼ºçƒˆ', 'significant', 'major', 'critical']):
            impact_score = 0.8
        elif any(word in response_lower for word in ['è½»å¾®', 'minor', 'limited', 'small']):
            impact_score = 0.3
        
        if any(word in response_lower for word in ['ç§¯æ', 'positive', 'bullish', 'up']):
            sentiment = 'positive'
        elif any(word in response_lower for word in ['æ¶ˆæ', 'negative', 'bearish', 'down']):
            sentiment = 'negative'
        
        return AIAnalysisResult(
            impact_score=impact_score,
            market_prediction=response[:200] + "..." if len(response) > 200 else response,
            trading_suggestion="è¯·åŸºäºAIåˆ†æç»“æœè°¨æ…æŠ•èµ„",
            sentiment=sentiment,
            confidence=0.6,
            key_points=["AIæ–‡æœ¬åˆ†æç»“æœ"]
        )

class FinancialNewsMonitor:
    def __init__(self):
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        global ENHANCED_MODE
        if ENHANCED_MODE:
            try:
                logger.debug("å¼€å§‹åˆ›å»ºConfigManager...")
                self.config_manager = ConfigManager()
                logger.debug("ConfigManageråˆ›å»ºæˆåŠŸ")
                
                logger.debug("å¼€å§‹åŠ è½½å¢å¼ºé…ç½®...")
                self.config = self._load_enhanced_config()
                logger.debug("å¢å¼ºé…ç½®åŠ è½½æˆåŠŸ")
                
                logger.debug("å¼€å§‹åˆ›å»ºFreeDataCollector...")
                self.free_data_collector = FreeDataCollector(self.config_manager)
                logger.debug("FreeDataCollectoråˆ›å»ºæˆåŠŸ")
                
                logger.info("å¢å¼ºé…ç½®åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.error(f"å¢å¼ºé…ç½®åŠ è½½å¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                self.config = self._get_default_config()
                ENHANCED_MODE = False
        else:
            self.config = self._get_default_config()
        
        # åˆå§‹åŒ–AIåˆ†æå™¨
        self.ai_analyzer = AIFinancialAnalyzer(self.config['ai_model'])
        
        # åˆå§‹åŒ–å…³é”®è¯å’Œç¼“å­˜
        self._init_keywords_and_cache()
    
    def _load_enhanced_config(self):
        """åŠ è½½å¢å¼ºé…ç½®"""
        logger.debug("å¼€å§‹è·å–AIæ¨¡å‹é…ç½®...")
        ai_models = self.config_manager.get_ai_models()
        logger.debug("å¼€å§‹è·å–è®¤è¯æ•°æ®æºé…ç½®...")
        auth_sources = self.config_manager.get_authenticated_sources()
        logger.debug("é…ç½®è·å–å®Œæˆ")
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„AIæ¨¡å‹
        ai_model_config = None
        if ai_models:
            model_config = ai_models[0]  # å·²ç»æŒ‰ä¼˜å…ˆçº§æ’åº
            ai_model_config = {
                'type': model_config['type'],
                'api_key': model_config.get('api_key', ''),
                'model': model_config.get('model', ''),
                'api_url': model_config.get('base_url', '')
            }
        
        if not ai_model_config:
            ai_model_config = self._get_default_ai_config()
        
        return {
            'ai_model': ai_model_config,
            'twitter_api_key': auth_sources.get('twitter', {}).get('api_key', ''),
            'twitter_api_secret': auth_sources.get('twitter', {}).get('api_secret', ''),
            'twitter_access_token': auth_sources.get('twitter', {}).get('access_token', ''),
            'twitter_access_token_secret': auth_sources.get('twitter', {}).get('access_token_secret', ''),
            'twitter_bearer_token': auth_sources.get('twitter', {}).get('bearer_token', ''),
            'youtube_api_key': auth_sources.get('youtube', {}).get('api_key', ''),
            'news_api_key': auth_sources.get('news_api', {}).get('api_key', ''),
        }
    
    def _get_default_config(self):
        """è·å–é»˜è®¤é…ç½®"""
        return {
            # æ•°æ®æºAPIé…ç½®
            'twitter_api_key': 'YOUR_TWITTER_API_KEY',
            'twitter_api_secret': 'YOUR_TWITTER_API_SECRET',
            'twitter_access_token': 'YOUR_ACCESS_TOKEN',
            'twitter_access_token_secret': 'YOUR_ACCESS_TOKEN_SECRET',
            'twitter_bearer_token': 'YOUR_BEARER_TOKEN',
            'youtube_api_key': 'YOUR_YOUTUBE_API_KEY',
            'news_api_key': 'YOUR_NEWS_API_KEY',
            'ai_model': self._get_default_ai_config()
        }
    
    def _get_default_ai_config(self):
        """è·å–é»˜è®¤AIé…ç½®"""
        return {
            'type': 'openai',  # å¯é€‰: 'openai', 'claude', 'custom'
            'api_key': 'YOUR_OPENAI_API_KEY',
            'model': 'gpt-4'  # æˆ– 'gpt-3.5-turbo'
        }
    
    def _init_keywords_and_cache(self):
        """åˆå§‹åŒ–å…³é”®è¯å’Œç¼“å­˜"""
        # æ™ºèƒ½å…³é”®è¯é…ç½® - ç”¨äºåˆæ­¥ç­›é€‰
        self.smart_keywords = {
            'high_priority': [
                # å¤®è¡Œå’Œè´§å¸æ”¿ç­–
                'federal reserve', 'fed', 'interest rate', 'inflation', 'monetary policy',
                'ç¾è”å‚¨', 'å¤®è¡Œ', 'åˆ©ç‡', 'é€šèƒ€', 'è´§å¸æ”¿ç­–',
                
                # åœ°ç¼˜æ”¿æ²»
                'war', 'conflict', 'sanctions', 'trade war', 'geopolitical',
                'æˆ˜äº‰', 'å†²çª', 'åˆ¶è£', 'è´¸æ˜“æˆ˜', 'åœ°ç¼˜æ”¿æ²»',
                
                # é‡å¤§ç»æµäº‹ä»¶
                'recession', 'crisis', 'bailout', 'stimulus', 'gdp',
                'è¡°é€€', 'å±æœº', 'æ•‘åŠ©', 'åˆºæ¿€', 'ç»æµ'
            ],
            
            'medium_priority': [
                # åŠ å¯†è´§å¸
                'bitcoin', 'ethereum', 'crypto', 'blockchain', 'defi',
                'æ¯”ç‰¹å¸', 'ä»¥å¤ªåŠ', 'åŠ å¯†è´§å¸', 'åŒºå—é“¾',
                
                # ç§‘æŠ€è‚¡
                'tech stock', 'ai', 'artificial intelligence', 'metaverse',
                'ç§‘æŠ€è‚¡', 'äººå·¥æ™ºèƒ½', 'å…ƒå®‡å®™',
                
                # èƒ½æºå’Œå•†å“
                'oil price', 'gold', 'commodity', 'energy',
                'æ²¹ä»·', 'é»„é‡‘', 'å¤§å®—å•†å“', 'èƒ½æº'
            ],
            
            'company_focus': [
                # é‡è¦å…¬å¸å’ŒCEO
                'apple', 'microsoft', 'tesla', 'amazon', 'google',
                'elon musk', 'warren buffett', 'jerome powell',
                'è‹¹æœ', 'å¾®è½¯', 'ç‰¹æ–¯æ‹‰', 'äºšé©¬é€Š', 'è°·æ­Œ'
            ]
        }
        
        self.news_cache = []
    
    async def get_free_rss_news(self) -> List[Dict]:
        """è·å–å…è´¹RSSæ–°é—»"""
        if not (ENHANCED_MODE and hasattr(self, 'free_data_collector')):
            return []
        
        try:
            # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            async with self.free_data_collector as collector:
                rss_data = await collector.collect_rss_feeds()
                news_items = []
                
                for item in rss_data:
                    # åº”ç”¨æ™ºèƒ½å…³é”®è¯è¿‡æ»¤
                    is_relevant, relevance_level = self.smart_keyword_filter(item.get('title', '') + ' ' + item.get('description', ''))
                    
                    if is_relevant:
                        news_items.append({
                            'source': f"RSS/{item.get('source', 'Unknown')}",
                            'content': f"{item.get('title', '')}. {item.get('description', '')}",
                            'timestamp': item.get('published', datetime.now().isoformat()),
                            'url': item.get('link', ''),
                            'priority': 'medium',
                            'relevance': relevance_level
                        })
                
                logger.info(f"RSSæ•°æ®æ”¶é›†å®Œæˆï¼Œè·å¾— {len(news_items)} æ¡ç›¸å…³æ–°é—»")
                return news_items
            
        except Exception as e:
            logger.error(f"è·å–RSSæ–°é—»å¤±è´¥: {e}")
            return []
    
    async def get_free_crypto_news(self) -> List[Dict]:
        """è·å–å…è´¹åŠ å¯†è´§å¸æ–°é—»"""
        if not (ENHANCED_MODE and hasattr(self, 'free_data_collector')):
            return []
        
        try:
            # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            async with self.free_data_collector as collector:
                crypto_data = await collector.collect_crypto_data()
                news_items = []
                
                # å¤„ç†ä»·æ ¼å˜åŠ¨æ–°é—»
                for coin, data in crypto_data.items():
                    if data.get('price_change_24h', 0) != 0:
                        change_pct = data.get('price_change_percentage_24h', 0)
                        
                        # åªå…³æ³¨æ˜¾è‘—å˜åŒ–
                        if abs(change_pct) >= 5:
                            content = f"{coin.upper()} ä»·æ ¼24å°æ—¶å˜åŠ¨ {change_pct:.2f}%ï¼Œå½“å‰ä»·æ ¼ ${data.get('current_price', 0):.4f}"
                            
                            news_items.append({
                                'source': 'CoinGecko/ä»·æ ¼ç›‘æ§',
                                'content': content,
                                'timestamp': datetime.now().isoformat(),
                                'url': f"https://www.coingecko.com/en/coins/{coin}",
                                'priority': 'high' if abs(change_pct) >= 10 else 'medium',
                                'relevance': 'high'
                            })
                
                logger.info(f"åŠ å¯†è´§å¸æ•°æ®æ”¶é›†å®Œæˆï¼Œè·å¾— {len(news_items)} æ¡ä»·æ ¼å˜åŠ¨æ–°é—»")
                return news_items
            
        except Exception as e:
            logger.error(f"è·å–åŠ å¯†è´§å¸æ–°é—»å¤±è´¥: {e}")
            return []
    
    async def get_free_market_news(self) -> List[Dict]:
        """è·å–å…è´¹å¸‚åœºæ–°é—»"""
        if not (ENHANCED_MODE and hasattr(self, 'free_data_collector')):
            return []
        
        try:
            # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨
            async with self.free_data_collector as collector:
                market_data = await collector.collect_market_data()
                news_items = []
                
                # å¤„ç†å¸‚åœºæ•°æ®
                for item in market_data:
                    is_relevant, relevance_level = self.smart_keyword_filter(item.get('content', ''))
                    
                    if is_relevant:
                        news_items.append({
                            'source': f"å…è´¹å¸‚åœºæ•°æ®/{item.get('source', 'Unknown')}",
                            'content': item.get('content', ''),
                            'timestamp': item.get('timestamp', datetime.now().isoformat()),
                            'url': item.get('url', ''),
                            'priority': 'medium',
                            'relevance': relevance_level
                        })
                
                logger.info(f"å¸‚åœºæ•°æ®æ”¶é›†å®Œæˆï¼Œè·å¾— {len(news_items)} æ¡ç›¸å…³æ–°é—»")
                return news_items
            
        except Exception as e:
            logger.error(f"è·å–å¸‚åœºæ–°é—»å¤±è´¥: {e}")
            return []
        
    def setup_twitter_api(self):
        """è®¾ç½®Twitter API"""
        try:
            auth = tweepy.OAuthHandler(
                self.config['twitter_api_key'], 
                self.config['twitter_api_secret']
            )
            auth.set_access_token(
                self.config['twitter_access_token'], 
                self.config['twitter_access_token_secret']
            )
            
            self.twitter_api = tweepy.API(auth, wait_on_rate_limit=True)
            self.twitter_client = tweepy.Client(
                bearer_token=self.config['twitter_bearer_token'],
                wait_on_rate_limit=True
            )
            logger.info("Twitter API é…ç½®æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"Twitter API é…ç½®å¤±è´¥: {e}")
            return False
    
    def smart_keyword_filter(self, text: str) -> Tuple[bool, str]:
        """æ™ºèƒ½å…³é”®è¯è¿‡æ»¤ - ç¡®å®šæ–°é—»ä¼˜å…ˆçº§"""
        text_lower = text.lower()
        
        # æ£€æŸ¥é«˜ä¼˜å…ˆçº§å…³é”®è¯
        for keyword in self.smart_keywords['high_priority']:
            if keyword.lower() in text_lower:
                return True, 'high'
        
        # æ£€æŸ¥ä¸­ä¼˜å…ˆçº§å…³é”®è¯
        for keyword in self.smart_keywords['medium_priority']:
            if keyword.lower() in text_lower:
                return True, 'medium'
        
        # æ£€æŸ¥å…¬å¸ç„¦ç‚¹å…³é”®è¯
        for keyword in self.smart_keywords['company_focus']:
            if keyword.lower() in text_lower:
                return True, 'company'
        
        return False, 'none'
    
    async def get_enhanced_twitter_news(self) -> List[Dict]:
        """å¢å¼ºç‰ˆTwitteræ–°é—»è·å–"""
        if not hasattr(self, 'twitter_client'):
            return []
            
        news_items = []
        
        # åˆ†çº§è´¦æˆ·ç›‘æ§
        priority_accounts = {
            'critical': ['federalreserve', 'SecYellen', 'WSJ', 'Reuters'],
            'important': ['elonmusk', 'POTUS', 'IMFNews', 'BloombergNews'],
            'watchlist': ['RayDalio', 'naval', 'APComplexity', 'SatoshiLite']
        }
        
        try:
            for priority, accounts in priority_accounts.items():
                for account in accounts:
                    # è·å–ç”¨æˆ·ä¿¡æ¯
                    user = self.twitter_client.get_user(username=account)
                    if user.data:
                        # è·å–ç”¨æˆ·æ¨æ–‡
                        tweets = self.twitter_client.get_users_tweets(
                            id=user.data.id,
                            max_results=15,
                            tweet_fields=['created_at', 'public_metrics', 'context_annotations'],
                            exclude='retweets'
                        )
                    
                    if tweets.data:
                        for tweet in tweets.data:
                            is_relevant, relevance_level = self.smart_keyword_filter(tweet.text)
                            
                            if is_relevant:
                                news_items.append({
                                    'source': f'Twitter/@{account}',
                                    'content': tweet.text,
                                    'timestamp': tweet.created_at.isoformat(),
                                    'url': f'https://twitter.com/{account}/status/{tweet.id}',
                                    'priority': priority,
                                    'relevance': relevance_level,
                                    'engagement': tweet.public_metrics
                                })
        except Exception as e:
            logger.error(f"è·å–Twitteræ•°æ®å¤±è´¥: {e}")
        
        return news_items
    
    async def get_ai_filtered_news(self) -> List[Dict]:
        """AIè¾…åŠ©è¿‡æ»¤çš„æ–°é—»è·å–"""
        news_items = []
        
        # ä½¿ç”¨æ›´ç²¾å‡†çš„æœç´¢å…³é”®è¯
        search_queries = [
            'stock market crash OR rally',
            'federal reserve interest rates',
            'cryptocurrency regulation bitcoin',
            'inflation CPI data economy',
            'geopolitical crisis war sanctions',
            'earnings report beat miss guidance'
        ]
        
        for query in search_queries:
            try:
                url = 'https://newsapi.org/v2/everything'
                params = {
                    'apiKey': self.config['news_api_key'],
                    'q': query,
                    'language': 'en',
                    'sortBy': 'publishedAt',
                    'pageSize': 10,
                    'from': (datetime.now() - timedelta(hours=12)).isoformat()
                }
                
                response = requests.get(url, params=params)
                data = response.json()
                
                if data['status'] == 'ok':
                    for article in data['articles']:
                        content = article['title'] + '. ' + (article['description'] or '')
                        is_relevant, relevance_level = self.smart_keyword_filter(content)
                        
                        if is_relevant:
                            news_items.append({
                                'source': article['source']['name'],
                                'content': content,
                                'timestamp': article['publishedAt'],
                                'url': article['url'],
                                'query': query,
                                'relevance': relevance_level
                            })
                            
            except Exception as e:
                logger.error(f"è·å–æ–°é—»APIæ•°æ®å¤±è´¥ (æŸ¥è¯¢: {query}): {e}")
        
        return news_items
    
    def get_news_api_data(self) -> List[Dict]:
        """è·å–æ–°é—»APIæ•°æ®"""
        news_items = []
        
        # ä½¿ç”¨NewsAPIè·å–è´¢ç»æ–°é—»
        url = 'https://newsapi.org/v2/everything'
        params = {
            'apiKey': self.config['news_api_key'],
            'q': 'stock market OR cryptocurrency OR federal reserve OR geopolitical',
            'language': 'en',
            'sortBy': 'publishedAt',
            'pageSize': 50,
            'from': (datetime.now() - timedelta(hours=24)).isoformat()
        }
        
        try:
            response = requests.get(url, params=params)
            data = response.json()
            
            if data['status'] == 'ok':
                for article in data['articles']:
                    if self.is_market_relevant(article['title'] + ' ' + (article['description'] or '')):
                        news_items.append({
                            'source': article['source']['name'],
                            'content': article['title'] + '. ' + (article['description'] or ''),
                            'timestamp': article['publishedAt'],
                            'url': article['url']
                        })
        except Exception as e:
            logger.error(f"è·å–æ–°é—»APIæ•°æ®å¤±è´¥: {e}")
        
        return news_items
    
    async def get_youtube_news(self) -> List[Dict]:
        """è·å–YouTubeæ–°é—»"""
        news_items = []
        
        # é‡è¦çš„è´¢ç»YouTubeé¢‘é“
        channels = [
            'UCrp_UI8XtuYfpiqluWLD7Lw',  # CNBC
            'UCF9IOB2TExg3QIBupFtBDxg',  # BBC News
            'UCupvZG-5ko_eiXAupbDfxWw'   # CNN
        ]
        
        try:
            for channel_id in channels:
                url = f'https://www.googleapis.com/youtube/v3/search'
                params = {
                    'key': self.config['youtube_api_key'],
                    'channelId': channel_id,
                    'part': 'snippet',
                    'order': 'date',
                    'maxResults': 10,
                    'publishedAfter': (datetime.now() - timedelta(hours=24)).isoformat()
                }
                
                response = requests.get(url, params=params)
                data = response.json()
                
                if 'items' in data:
                    for item in data['items']:
                        title = item['snippet']['title']
                        description = item['snippet']['description']
                        
                        if self.is_market_relevant(title + ' ' + description):
                            news_items.append({
                                'source': f'YouTube/{item["snippet"]["channelTitle"]}',
                                'content': f'{title}. {description[:200]}...',
                                'timestamp': item['snippet']['publishedAt'],
                                'url': f'https://www.youtube.com/watch?v={item["id"]["videoId"]}'
                            })
        except Exception as e:
            logger.error(f"è·å–YouTubeæ•°æ®å¤±è´¥: {e}")
        
        return news_items
    
    def is_market_relevant(self, text: str) -> bool:
        """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸å¸‚åœºç›¸å…³ - å¢å¼ºç‰ˆç›¸å…³æ€§æ£€æµ‹"""
        text_lower = text.lower()
        relevance_score = 0
        
        # 1. åŸºç¡€å…³é”®è¯æ£€æµ‹
        for category, keywords in self.keywords.items():
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    relevance_score += self.impact_weights[category]
        
        # 2. é‡‘èæœ¯è¯­æ£€æµ‹
        financial_terms = [
            'stock', 'market', 'trading', 'investment', 'portfolio', 'dividend',
            'earnings', 'revenue', 'profit', 'loss', 'valuation', 'ipo',
            'è‚¡ç¥¨', 'å¸‚åœº', 'äº¤æ˜“', 'æŠ•èµ„', 'æ”¶ç›Š', 'äºæŸ', 'ä¼°å€¼'
        ]
        
        for term in financial_terms:
            if term in text_lower:
                relevance_score += 0.3
        
        # 3. ç»æµæŒ‡æ ‡æ£€æµ‹
        economic_indicators = [
            'gdp', 'unemployment', 'cpi', 'ppi', 'pmi', 'retail sales',
            'consumer confidence', 'housing starts', 'å¤±ä¸šç‡', 'é€šèƒ€ç‡', 'gdp'
        ]
        
        for indicator in economic_indicators:
            if indicator in text_lower:
                relevance_score += 0.4
        
        # 4. å…¬å¸è´¢æŠ¥ç›¸å…³
        earnings_terms = [
            'quarterly results', 'annual report', 'guidance', 'outlook',
            'beat estimates', 'miss estimates', 'è´¢æŠ¥', 'ä¸šç»©', 'é¢„æœŸ'
        ]
        
        for term in earnings_terms:
            if term in text_lower:
                relevance_score += 0.5
        
        # 5. ç›‘ç®¡æ”¿ç­–ç›¸å…³
        regulatory_terms = [
            'regulation', 'policy', 'law', 'compliance', 'sec', 'fda',
            'antitrust', 'merger', 'acquisition', 'ç›‘ç®¡', 'æ”¿ç­–', 'æ³•è§„'
        ]
        
        for term in regulatory_terms:
            if term in text_lower:
                relevance_score += 0.4
        
        # é˜ˆå€¼åˆ¤æ–­ï¼šç›¸å…³æ€§åˆ†æ•°å¤§äº0.5æ‰è®¤ä¸ºä¸å¸‚åœºç›¸å…³
        return relevance_score >= 0.5
    
    def analyze_market_impact(self, text: str) -> Tuple[float, str, str]:
        """åˆ†æå¸‚åœºå½±å“ - å¢å¼ºç‰ˆå½±å“è¯„ä¼°ç®—æ³•"""
        text_lower = text.lower()
        impact_score = 0.0
        categories_found = []
        
        # 1. åŸºç¡€å…³é”®è¯åŒ¹é…å’ŒåŠ æƒ
        for category, keywords in self.keywords.items():
            category_score = 0
            for keyword in keywords:
                if keyword.lower() in text_lower:
                    category_score += 1
            
            if category_score > 0:
                categories_found.append(category)
                impact_score += category_score * self.impact_weights[category]
        
        # 2. æƒ…æ„Ÿåˆ†æå¢å¼º - æ£€æµ‹ç§¯æ/æ¶ˆæè¯æ±‡
        positive_words = ['rise', 'up', 'bull', 'growth', 'increase', 'boost', 'surge', 'ä¸Šæ¶¨', 'å¢é•¿', 'çœ‹æ¶¨']
        negative_words = ['fall', 'down', 'bear', 'crash', 'decline', 'drop', 'plunge', 'ä¸‹è·Œ', 'æš´è·Œ', 'çœ‹è·Œ']
        
        sentiment_multiplier = 1.0
        if any(word in text_lower for word in positive_words):
            sentiment_multiplier += 0.2
        if any(word in text_lower for word in negative_words):
            sentiment_multiplier += 0.3  # è´Ÿé¢æ–°é—»é€šå¸¸å½±å“æ›´å¤§
        
        # 3. ç´§æ€¥ç¨‹åº¦æ£€æµ‹
        urgency_words = ['breaking', 'urgent', 'emergency', 'crisis', 'alert', 'ç´§æ€¥', 'çªå‘', 'å±æœº']
        if any(word in text_lower for word in urgency_words):
            sentiment_multiplier += 0.4
        
        # 4. æ•°å­—/ç™¾åˆ†æ¯”æ£€æµ‹ - åŒ…å«å…·ä½“æ•°æ®çš„æ–°é—»å½±å“æ›´å¤§
        import re
        if re.search(r'\d+%|\$\d+|\d+\.\d+%', text):
            sentiment_multiplier += 0.2
        
        # 5. æ—¶é—´æ•æ„Ÿæ€§æ£€æµ‹
        time_sensitive = ['today', 'tonight', 'tomorrow', 'this week', 'ä»Šå¤©', 'ä»Šæ™š', 'æ˜å¤©', 'æœ¬å‘¨']
        if any(word in text_lower for word in time_sensitive):
            sentiment_multiplier += 0.2
        
        # åº”ç”¨æƒ…æ„Ÿå’Œç´§æ€¥ç¨‹åº¦ä¹˜æ•°
        impact_score *= sentiment_multiplier
        
        # 6. ä¿¡æ¯æºå¯ä¿¡åº¦è°ƒæ•´
        impact_score = self.adjust_for_source_credibility(impact_score, text)
        
        # æ ‡å‡†åŒ–å½±å“åˆ†æ•° (0-1)
        impact_score = min(impact_score / 15, 1.0)  # è°ƒæ•´åˆ†æ¯é€‚åº”æ–°çš„è¯„åˆ†ç³»ç»Ÿ
        
        # ç”Ÿæˆå¸‚åœºé¢„æµ‹
        market_prediction = self.generate_market_prediction(text_lower, categories_found, impact_score)
        
        # ç”Ÿæˆäº¤æ˜“å»ºè®®
        trading_suggestion = self.generate_trading_suggestion(text_lower, categories_found, impact_score)
        
        return impact_score, market_prediction, trading_suggestion
    
    def adjust_for_source_credibility(self, score: float, text: str) -> float:
        """æ ¹æ®ä¿¡æ¯æºå¯ä¿¡åº¦è°ƒæ•´å½±å“åˆ†æ•°"""
        # é«˜å¯ä¿¡åº¦æ¥æº
        high_credibility = ['reuters', 'bloomberg', 'wsj', 'financial times', 'cnbc', 'ap news']
        # ä¸­ç­‰å¯ä¿¡åº¦æ¥æº
        medium_credibility = ['cnn', 'bbc', 'guardian', 'nytimes']
        # ç¤¾äº¤åª’ä½“éœ€è¦æ›´è°¨æ…
        social_media = ['twitter', 'facebook', 'youtube']
        
        text_lower = text.lower()
        
        if any(source in text_lower for source in high_credibility):
            return score * 1.2  # æé«˜20%
        elif any(source in text_lower for source in medium_credibility):
            return score * 1.0  # ä¿æŒä¸å˜
        elif any(source in text_lower for source in social_media):
            return score * 0.8  # é™ä½20%
        
        return score
    
    def generate_market_prediction(self, text: str, categories: List[str], impact_score: float) -> str:
        """ç”Ÿæˆå¸‚åœºé¢„æµ‹"""
        if impact_score < 0.3:
            return "å¸‚åœºå½±å“è¾ƒå°ï¼Œé¢„è®¡æ³¢åŠ¨æœ‰é™"
        
        predictions = []
        
        if 'geopolitical' in categories:
            if any(word in text for word in ['war', 'conflict', 'æˆ˜äº‰', 'å†²çª']):
                predictions.append("åœ°ç¼˜æ”¿æ²»é£é™©ä¸Šå‡ï¼Œé¿é™©èµ„äº§å¯èƒ½å—ç›Šï¼Œé£é™©èµ„äº§æ‰¿å‹")
            
        if 'monetary' in categories:
            if any(word in text for word in ['rate cut', 'é™æ¯']):
                predictions.append("å®½æ¾è´§å¸æ”¿ç­–é¢„æœŸï¼Œè‚¡å¸‚å¯èƒ½ä¸Šæ¶¨ï¼Œç¾å…ƒèµ°å¼±")
            elif any(word in text for word in ['rate hike', 'raise rates', 'åŠ æ¯']):
                predictions.append("ç´§ç¼©è´§å¸æ”¿ç­–é¢„æœŸï¼Œè‚¡å¸‚å¯èƒ½ä¸‹è·Œï¼Œç¾å…ƒèµ°å¼º")
        
        if 'crypto' in categories:
            if any(word in text for word in ['regulation', 'ban', 'ç›‘ç®¡', 'ç¦æ­¢']):
                predictions.append("åŠ å¯†è´§å¸ç›‘ç®¡æ”¶ç´§ï¼Œæ•°å­—èµ„äº§å¯èƒ½ä¸‹è·Œ")
            elif any(word in text for word in ['adoption', 'institutional', 'é‡‡ç”¨', 'æœºæ„']):
                predictions.append("åŠ å¯†è´§å¸é‡‡ç”¨å¢åŠ ï¼Œæ•°å­—èµ„äº§å¯èƒ½ä¸Šæ¶¨")
        
        if not predictions:
            return "å¸‚åœºæƒ…ç»ªå¯èƒ½å‡ºç°æ³¢åŠ¨ï¼Œå»ºè®®å¯†åˆ‡å…³æ³¨åç»­å‘å±•"
        
        return "; ".join(predictions)
    
    def generate_trading_suggestion(self, text: str, categories: List[str], impact_score: float) -> str:
        """ç”Ÿæˆäº¤æ˜“å»ºè®®"""
        if impact_score < 0.2:
            return "æŒå¸è§‚æœ›ï¼Œæ— æ˜ç¡®äº¤æ˜“ä¿¡å·"
        
        suggestions = []
        
        # åŸºäºæ–°é—»å†…å®¹çš„äº¤æ˜“å»ºè®®é€»è¾‘
        if 'geopolitical' in categories:
            suggestions.append("è€ƒè™‘å¢æŒé»„é‡‘ã€ç¾å€ºç­‰é¿é™©èµ„äº§")
        
        if 'monetary' in categories:
            if any(word in text for word in ['dovish', 'cut', 'é¸½æ´¾', 'é™æ¯']):
                suggestions.append("å¯è€ƒè™‘ä¹°å…¥æˆé•¿è‚¡ã€ç§‘æŠ€è‚¡")
            elif any(word in text for word in ['hawkish', 'hike', 'é¹°æ´¾', 'åŠ æ¯']):
                suggestions.append("å¯è€ƒè™‘å‡æŒé«˜ä¼°å€¼è‚¡ç¥¨ï¼Œå¢æŒä»·å€¼è‚¡")
        
        if 'crypto' in categories:
            if any(word in text for word in ['positive', 'adoption', 'ç§¯æ', 'é‡‡ç”¨']):
                suggestions.append("å¯é€‚é‡é…ç½®ä¸»æµåŠ å¯†è´§å¸")
            else:
                suggestions.append("å»ºè®®å‡å°‘åŠ å¯†è´§å¸æ•å£")
        
        if not suggestions:
            return "å»ºè®®åˆ†æ•£æŠ•èµ„ï¼Œæ§åˆ¶é£é™©"
        
        return "; ".join(suggestions) + " (ä»…ä¾›å‚è€ƒï¼Œè¯·è°¨æ…æŠ•èµ„)"
    
    def format_news_output(self, news_item: NewsItem) -> str:
        """æ ¼å¼åŒ–æ–°é—»è¾“å‡º"""
        timestamp = datetime.fromisoformat(news_item.timestamp.replace('Z', '+00:00'))
        formatted_time = timestamp.strftime('%Y-%m-%d %H:%M:%S')
        
        return f"""
ğŸ“ˆ ã€è´¢ç»æ–°é—»æ’­æŠ¥ã€‘
ğŸ• æ—¶é—´: {formatted_time}
ğŸ“º æ¥æº: {news_item.source}
ğŸ“° äº‹ä»¶: {news_item.title}
ğŸ“Š å½±å“åˆ†æ: {news_item.market_prediction}
ğŸ’¡ äº¤æ˜“å»ºè®®: {news_item.trading_suggestion}
ğŸ”— é“¾æ¥: {news_item.url}
{"="*60}
"""
    
    async def collect_and_analyze_news(self) -> List[NewsItem]:
        """æ”¶é›†å¹¶åˆ†ææ‰€æœ‰æ–°é—» - AIå¢å¼ºç‰ˆ"""
        all_raw_news = []
        
        logger.info("å¼€å§‹æ”¶é›†æ–°é—»æ•°æ®...")
        
        # 1. å¹¶å‘æ”¶é›†å„å¹³å°æ•°æ®
        tasks = [
            self.get_enhanced_twitter_news(),
            self.get_ai_filtered_news(),
            self.get_youtube_news()
        ]
        
        # æ·»åŠ å…è´¹æ•°æ®æºï¼ˆå¦‚æœå¢å¼ºæ¨¡å¼å¯ç”¨ï¼‰
        if ENHANCED_MODE and hasattr(self, 'free_data_collector'):
            tasks.append(self.get_free_rss_news())
            tasks.append(self.get_free_crypto_news())
            tasks.append(self.get_free_market_news())
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, list):
                all_raw_news.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"æ•°æ®æ”¶é›†ä»»åŠ¡å¤±è´¥: {result}")
        
        logger.info(f"åŸå§‹æ–°é—»æ”¶é›†å®Œæˆï¼Œå…± {len(all_raw_news)} æ¡")
        
        # 2. å»é‡å’Œåˆæ­¥è¿‡æ»¤
        unique_news = self.deduplicate_news(all_raw_news)
        logger.info(f"å»é‡åå‰©ä½™ {len(unique_news)} æ¡æ–°é—»")
        
        # 3. AIåˆ†æé˜¶æ®µ
        analyzed_news = []
        analysis_tasks = []
        
        # ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§æ–°é—»
        high_priority_news = []
        medium_priority_news = []
        
        for n in unique_news:
            if n.get('priority') == 'critical' or n.get('relevance') == 'high':
                high_priority_news.append(n)
            else:
                medium_priority_news.append(n)
        
        # åˆ†æ‰¹è¿›è¡ŒAIåˆ†æä»¥æ§åˆ¶APIè°ƒç”¨æˆæœ¬
        max_analysis_count = 20  # æ¯æ¬¡æœ€å¤šåˆ†æ20æ¡æ–°é—»
        news_to_analyze = (high_priority_news + medium_priority_news)[:max_analysis_count]
        
        logger.info(f"å¼€å§‹AIåˆ†æï¼Œå¤„ç† {len(news_to_analyze)} æ¡æ–°é—»")
        
        for news_item in news_to_analyze:
            try:
                # å¼‚æ­¥è°ƒç”¨AIåˆ†æ
                ai_analysis = await self.ai_analyzer.analyze_news_with_ai(
                    news_item['content'], 
                    news_item['source']
                )
                
                # åªä¿ç•™AIè®¤ä¸ºæœ‰è¶³å¤Ÿå½±å“çš„æ–°é—»
                if ai_analysis.impact_score >= 0.3:
                    news_obj = NewsItem(
                        timestamp=news_item['timestamp'],
                        source=news_item['source'],
                        title=news_item['content'][:150] + "..." if len(news_item['content']) > 150 else news_item['content'],
                        content=news_item['content'],
                        url=news_item['url'],
                        ai_analysis=ai_analysis
                    )
                    analyzed_news.append(news_obj)
                
                # æ·»åŠ å»¶è¿Ÿä»¥é¿å…APIé™åˆ¶
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"AIåˆ†ææ–°é—»å¤±è´¥: {e}")
                continue
        
        # æŒ‰AIè¯„åˆ†æ’åº
        analyzed_news.sort(key=lambda x: x.ai_analysis.impact_score if x.ai_analysis else 0, reverse=True)
        
        logger.info(f"AIåˆ†æå®Œæˆï¼Œç­›é€‰å‡º {len(analyzed_news)} æ¡é‡è¦æ–°é—»")
        return analyzed_news
    
    def deduplicate_news(self, news_list: List[Dict]) -> List[Dict]:
        """æ–°é—»å»é‡"""
        seen_content = set()
        unique_news = []
        
        for news in news_list:
            # ä½¿ç”¨å†…å®¹çš„å‰100ä¸ªå­—ç¬¦ä½œä¸ºå»é‡æ ‡è¯†
            content_key = news['content'][:100].lower().strip()
            
            if content_key not in seen_content:
                seen_content.add(content_key)
                unique_news.append(news)
        
        return unique_news
    
    def format_ai_news_output(self, news_item: NewsItem) -> str:
        """æ ¼å¼åŒ–AIåˆ†æçš„æ–°é—»è¾“å‡º"""
        if not news_item.ai_analysis:
            return f"æ–°é—»: {news_item.title} (AIåˆ†æå¤±è´¥)"
        
        timestamp = datetime.fromisoformat(news_item.timestamp.replace('Z', '+00:00'))
        formatted_time = timestamp.strftime('%Y-%m-%d %H:%M:%S')
        
        # å½±å“ç­‰çº§æ˜¾ç¤º
        impact_level = "ğŸ”´ é«˜å½±å“" if news_item.ai_analysis.impact_score >= 0.7 else \
                     "ğŸŸ¡ ä¸­å½±å“" if news_item.ai_analysis.impact_score >= 0.4 else "ğŸŸ¢ ä½å½±å“"
        
        # æƒ…æ„Ÿå›¾æ ‡
        sentiment_icon = "ğŸ“ˆ" if news_item.ai_analysis.sentiment == "positive" else \
                        "ğŸ“‰" if news_item.ai_analysis.sentiment == "negative" else "â¡ï¸"
        
        output = f"""
ğŸ¤– ã€AIè´¢ç»æ–°é—»åˆ†æã€‘
ğŸ• æ—¶é—´: {formatted_time}
ğŸ“º æ¥æº: {news_item.source}
{impact_level} å½±å“è¯„åˆ†: {news_item.ai_analysis.impact_score:.2f}
{sentiment_icon} å¸‚åœºæƒ…æ„Ÿ: {news_item.ai_analysis.sentiment}
ğŸ¯ AIä¿¡å¿ƒåº¦: {news_item.ai_analysis.confidence:.2f}

ğŸ“° æ–°é—»æ‘˜è¦: {news_item.title}

ğŸ” AIå¸‚åœºåˆ†æ:
{news_item.ai_analysis.market_prediction}

ğŸ’¡ AIäº¤æ˜“å»ºè®®:
{news_item.ai_analysis.trading_suggestion}
"""
        
        if news_item.ai_analysis.key_points:
            output += f"\nğŸ“Œ å…³é”®è¦ç‚¹:\n"
            for i, point in enumerate(news_item.ai_analysis.key_points[:3], 1):
                output += f"   {i}. {point}\n"
        
        output += f"\nğŸ”— åŸæ–‡é“¾æ¥: {news_item.url}\n"
        output += "="*70 + "\n"
        
        return output
    
    def save_news_to_file(self, news_items: List[NewsItem], filename: str = None):
        """ä¿å­˜æ–°é—»åˆ°æ–‡ä»¶"""
        if not filename:
            filename = f"financial_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(f"è´¢ç»æ–°é—»æ’­æŠ¥ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
            
            for news in news_items:
                f.write(self.format_news_output(news))
                f.write("\n")
        
        logger.info(f"æ–°é—»å·²ä¿å­˜åˆ°æ–‡ä»¶: {filename}")
    
    async def run_monitor(self, interval_minutes: int = 30):
        """è¿è¡Œç›‘æ§ç¨‹åº"""
        logger.info(f"å¯åŠ¨è´¢ç»æ–°é—»ç›‘æ§ï¼Œæ£€æŸ¥é—´éš”: {interval_minutes}åˆ†é’Ÿ")
        
        # åˆå§‹åŒ–API
        self.setup_twitter_api()
        
        while True:
            try:
                logger.info("å¼€å§‹æ–°ä¸€è½®æ–°é—»æ”¶é›†...")
                
                # æ”¶é›†æ–°é—»
                news_items = await self.collect_and_analyze_news()
                
                if news_items:
                    logger.info(f"å‘ç° {len(news_items)} æ¡é‡è¦æ–°é—»")
                    
                    # è¾“å‡ºæ–°é—»
                    for news in news_items[:10]:  # åªæ˜¾ç¤ºå‰10æ¡æœ€é‡è¦çš„
                        print(self.format_news_output(news))
                    
                    # ä¿å­˜åˆ°æ–‡ä»¶
                    self.save_news_to_file(news_items)
                else:
                    logger.info("æœªå‘ç°é‡è¦è´¢ç»æ–°é—»")
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                logger.info(f"ç­‰å¾… {interval_minutes} åˆ†é’Ÿåè¿›è¡Œä¸‹æ¬¡æ£€æŸ¥...")
                await asyncio.sleep(interval_minutes * 60)
                
            except KeyboardInterrupt:
                logger.info("ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
                break
            except Exception as e:
                logger.error(f"ç›‘æ§è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
                await asyncio.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿå†é‡è¯•

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ è´¢ç»æ–°é—»è‡ªåŠ¨æ’­æŠ¥ç³»ç»Ÿå¯åŠ¨")
    print("-" * 50)
    
    if ENHANCED_MODE:
        print("âœ… å¢å¼ºæ¨¡å¼å·²å¯ç”¨")
        print("ğŸ“Š å¯ç”¨æ•°æ®æº:")
        print("   - RSSæ–°é—»è®¢é˜… (å…è´¹)")
        print("   - CoinGeckoåŠ å¯†è´§å¸æ•°æ® (å…è´¹)")
        print("   - å…¬å¼€å¸‚åœºæ•°æ® (å…è´¹)")
        print("   - Twitter API (éœ€è¦å¯†é’¥)")
        print("   - YouTube API (éœ€è¦å¯†é’¥)")
        print("   - News API (éœ€è¦å¯†é’¥)")
        print("ğŸ’¡ å³ä½¿æ²¡æœ‰APIå¯†é’¥ä¹Ÿå¯ä»¥è·å–åŸºç¡€æ–°é—»æ•°æ®")
    else:
        print("âš ï¸  åŸºç¡€æ¨¡å¼ - ä½¿ç”¨å‰è¯·å…ˆé…ç½®ä»¥ä¸‹APIå¯†é’¥:")
        print("   - Twitter APIå¯†é’¥")
        print("   - YouTube APIå¯†é’¥") 
        print("   - News APIå¯†é’¥")
    
    print("-" * 50)
    
    monitor = FinancialNewsMonitor()
    
    # è¿è¡Œç›‘æ§
    try:
        asyncio.run(monitor.run_monitor(30))  # æ¯30åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    except KeyboardInterrupt:
        print("\nç¨‹åºå·²é€€å‡º")

if __name__ == "__main__":
    main()